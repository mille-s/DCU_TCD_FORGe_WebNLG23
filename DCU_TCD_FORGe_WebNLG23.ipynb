{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/DCU_TCD_FORGe_WebNLG23/blob/main/DCU_TCD_FORGe_WebNLG23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1EixVIRHmxqi"
      },
      "outputs": [],
      "source": [
        "# @title Prepare repo\n",
        "\n",
        "# Run this cell to download and unzip the working folder and install Java 8\n",
        "\n",
        "from IPython.display import clear_output, HTML, display\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# clone main repo\n",
        "! git clone https://github.com/mille-s/DCU_TCD-FORGe_WebNLG23.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'DCU_TCD-FORGe_WebNLG23/DCU_TCD_FORGe_WebNLG23.ipynb'\n",
        "\n",
        "# clone M-FleNS repo (generation pipeline)\n",
        "! git clone https://github.com/mille-s/M-FleNS_NLG-Pipeline.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'M-FleNS_NLG-Pipeline/M_FleNS_pipe_v2.ipynb'\n",
        "\n",
        "# Download FORGe\n",
        "# Version used for WebNLG (fails to generate a few structures of the training data)\n",
        "# ! gdown 1lsh8pwUp9mc0Z_aFbSy1WTIpSx9YwFFD\n",
        "# ! unzip /content/FORGe_colab_v3_WebNLG.zip\n",
        "# Version used for Mod-D2T (minor improvements on WebNLG)\n",
        "# ! gdown 196w_EtORTkR3idaXDMq0xl3pOtBrGbiE\n",
        "# ! unzip /content/FORGe_colab_v4.zip\n",
        "# Version used for GEM (now supporting some Wikidata properties)\n",
        "# ! gdown 1gaTZVGFjtR_zBNskJXCeIJVug95aGFkf\n",
        "# ! unzip /content/FORGe_colab_v5.zip\n",
        "# Version now supporting FR WebNLG data (including some minor improvements on aggregation)\n",
        "! gdown 1M0yk7aLUpHiT4UfT72g-rNIPd4W8WT44\n",
        "! unzip /content/FORGe_colab_v6.zip\n",
        "\n",
        "# Download triple to predArg conversion\n",
        "triple2predArg = 'triples2predArg'\n",
        "os.makedirs(triple2predArg)\n",
        "# ! gdown 1Fr_ThZHGPLkoi3XQthSsaM5uVGLxgVat\n",
        "# ! unzip 'triples2predArg2.zip' -d {triple2predArg}\n",
        "# ! rm 'triples2predArg2.zip'\n",
        "! gdown 1NKuoIqWj-VBUSCWos7k7Ps5gywVS0IGC\n",
        "! unzip 'triples2predArg3.zip' -d {triple2predArg}\n",
        "! rm 'triples2predArg3.zip'\n",
        "\n",
        "# Download Morphology generator\n",
        "! gdown 1vk1utEjeZ_2YO1H20DPDTjVSevgRJNM_\n",
        "morph_folder_name = 'test_irish_morph_gen_v5.0'\n",
        "zip_name = morph_folder_name+'.zip'\n",
        "! unzip {zip_name}\n",
        "\n",
        "morph_input_folder = '/content/'+morph_folder_name+'/Inputs'\n",
        "morph_output_folder = '/content/'+morph_folder_name+'/Outputs'\n",
        "os.makedirs(morph_input_folder)\n",
        "os.makedirs(morph_output_folder)\n",
        "\n",
        "# Make morphology flookup executable\n",
        "! 7z a -sfx {morph_folder_name}'/flookup.exe' {morph_folder_name}'/flookup'\n",
        "! chmod 755 {morph_folder_name}'/flookup'\n",
        "\n",
        "# Package for parsing XML files\n",
        "!pip install xmltodict\n",
        "# Install SPARQLWrapper\n",
        "! pip install SPARQLWrapper\n",
        "\n",
        "# Clean\n",
        "! rm '/content/FORGe_colab_v3_WebNLG.zip'\n",
        "! rm '/content/FORGe_colab_v4.zip'\n",
        "! rm '/content/FORGe_colab_v5.zip'\n",
        "! rm '/content/FORGe_colab_v6.zip'\n",
        "! rm '/content/test_irish_morph_gen_v5.0.zip'\n",
        "clear_output()\n",
        "print('Working folder ready!\\n--------------\\nInstalling Java 8...\\n')\n",
        "\n",
        "# Switch to Java 1.8 (needed for FORGe to run correctly)\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "  !java -version       #check java version\n",
        "install_java()\n",
        "\n",
        "# To wrap texts in cells\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sZrDTZNaPWkP"
      },
      "outputs": [],
      "source": [
        "# @title Set parameters, create empty folders\n",
        "import os\n",
        "# Run this cell to set parameters for generation\n",
        "\n",
        "# The input structure(s) of the correct type should be placed in the folder that corresponds to the first module called in the next cell\n",
        "# E.g. if one a module PredArg_... or DSynt_... is selected, the input predicate-argument structures should be placed in the structures/00-PredArg folder\n",
        "# I'll make the instructions and names clearer in a later (actually usable) version.\n",
        "\n",
        "############# Select language #############\n",
        "language = 'GA' #@param['EN', 'FR', 'GA']\n",
        "\n",
        "############# Choose whether or not to look for class and gender information of entities via DBpedia #############\n",
        "# Not implemented yet, right now it concatenates the files whatever happens\n",
        "get_class_gender = 'no' #@param['yes', 'no']\n",
        "\n",
        "############# Choose whether or not to concatenate output files #############\n",
        "# Not implemented yet, right now it concatenates the files whatever happens\n",
        "concatenate_output_files = 'yes' #@param['yes', 'no']\n",
        "\n",
        "############# Select module grouping #############\n",
        "# Group consecutive modules for the same system or call each module separately.\n",
        "# Select 'no' to get all intermediate representations, 'yes' if you're only interested in the output.\n",
        "generate_intermediate_representations = 'no' #@param['yes', 'no']\n",
        "group_modules_prm = ''\n",
        "if generate_intermediate_representations == 'yes':\n",
        "  group_modules_prm = 'no'\n",
        "else:\n",
        "  group_modules_prm = 'yes'\n",
        "\n",
        "############# Select dataset split #############\n",
        "split = \"dev\" #@param['dev', 'test','train','ukn']\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "# Modules to run, with type of processing (FORGe, Model1, SimpleNLG, etc.).\n",
        "# Only FORGe is supported for this prototype version.\n",
        "PredArg_Normalisation = 'FORGe'\n",
        "# To have an external module assigning triples to aggregate\n",
        "PredArg_AggregationMark = 'None'\n",
        "PredArg_Aggregation = 'FORGe'\n",
        "PredArg_PoSTagging = 'FORGe'\n",
        "PredArg_CommStructuring = 'FORGe'\n",
        "DSynt_Structuring = 'FORGe'\n",
        "SSynt_Structuring = 'FORGe'\n",
        "SSynt_Aggregation = 'FORGe'\n",
        "RE_Generation = 'FORGe'\n",
        "DMorph_AgreementsLinearisation = 'FORGe'\n",
        "SMorph_Processing = 'FORGe'\n",
        "\n",
        "# # Tests Lin only (also modify M-FleNS.py)\n",
        "# PredArg_Normalisation = 'None'\n",
        "# PredArg_AggregationMark = 'None'\n",
        "# PredArg_Aggregation = 'None'\n",
        "# PredArg_PoSTagging = 'None'\n",
        "# PredArg_CommStructuring = 'None'\n",
        "# DSynt_Structuring = 'None'\n",
        "# SSynt_Structuring = 'None'\n",
        "# SSynt_Aggregation = 'None'\n",
        "# RE_Generation = 'None'\n",
        "# DMorph_AgreementsLinearisation = 'FORGe'\n",
        "# SMorph_Processing = 'FORGe'\n",
        "\n",
        "#######################################################################\n",
        "# Paths to python files\n",
        "path_MFleNS = '/content/M-FleNS_NLG-Pipeline/code/M-FleNS.py'\n",
        "path_checkOutputs = '/content/M-FleNS_NLG-Pipeline/code/M-FleNS-checkOutputs.py'\n",
        "path_postProc = '/content/M-FleNS_NLG-Pipeline/code/postProcess.py'\n",
        "path_FORGe2Morph = '/content/DCU_TCD-FORGe_WebNLG23/code/FORGe2Morph.py'\n",
        "path_concatenate = '/content/M-FleNS_NLG-Pipeline/code/concatenate_files.py'\n",
        "path_getClassGenderDBp = '/content/M-FleNS_NLG-Pipeline/code/getClassGenderDBpedia.py'\n",
        "path_splitFiles = '/content/M-FleNS_NLG-Pipeline/code/splitFiles.py'\n",
        "# path_MorphGen = '/content/DCU_TCD-FORGe_WebNLG23/code/IrishNLP_MorphGen.py'\n",
        "\n",
        "#######################################################################\n",
        "# Paths to FORGe/MATE folders and property files\n",
        "FORGe_input_folder = '/content/FORGe/buddy_project/struct'\n",
        "path_MATE = '/content/FORGe/buddy-patched.jar'\n",
        "path_props_resources_template = '/content/FORGe/mateColabDrive.properties'\n",
        "path_props_levels = '/content/FORGe/mateLevels.properties'\n",
        "path_props = '/content/FORGe/mate.properties'\n",
        "\n",
        "# Paths to general folders\n",
        "# The input structure(s) of the correct type should be placed in the folder that corresponds to the first module called in the next cell\n",
        "path_strs = '/content/FORGe/structures'\n",
        "path_input_XML = '/content/input_XMLs'\n",
        "str_PredArg_folder = os.path.join(path_strs, '00-PredArg')\n",
        "str_PredArgNorm_folder = os.path.join(path_strs, '01-PredArgNorm')\n",
        "str_PredArgAggMark_folder = os.path.join(path_strs, '02-PredArgAggMark')\n",
        "str_PredArgAgg_folder = os.path.join(path_strs, '03-PredArgAgg')\n",
        "str_PredArgPoS_folder = os.path.join(path_strs, '04-PredArgPoS')\n",
        "str_PredArgComm_folder = os.path.join(path_strs, '05-PredArgComm')\n",
        "str_DSynt_folder = os.path.join(path_strs, '06-DSynt')\n",
        "str_SSynt_folder = os.path.join(path_strs, '07-SSynt')\n",
        "str_SSyntAgg_folder = os.path.join(path_strs, '08-SSyntAgg')\n",
        "str_REG_folder = os.path.join(path_strs, '09-REG')\n",
        "str_DMorphLin_folder = os.path.join(path_strs, '10-DMorphLin')\n",
        "str_SMorphText_folder = os.path.join(path_strs, '11-SMorphText')\n",
        "log_folder = '/content/FORGe/log'\n",
        "\n",
        "if not os.path.exists(log_folder):\n",
        "  os.makedirs(log_folder)\n",
        "if not os.path.exists(path_input_XML):\n",
        "  os.makedirs(path_input_XML)\n",
        "temp_input_folder_morph = '/content/FORGe-out'\n",
        "if not os.path.exists(temp_input_folder_morph):\n",
        "  os.makedirs(temp_input_folder_morph)\n",
        "\n",
        "def clear_files(folder):\n",
        "  \"Function to clear files from a folder.\"\n",
        "  if os.path.exists(folder) and os.path.isdir(folder):\n",
        "    for filename in os.listdir(folder):\n",
        "      file_path = os.path.join(folder, filename)\n",
        "      try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "          os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "          shutil.rmtree(file_path)\n",
        "      except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "def clear_folder(folder):\n",
        "  \"Function to clear whole folders.\"\n",
        "  if os.path.exists(folder) and os.path.isdir(folder):\n",
        "    try:\n",
        "      shutil.rmtree(folder)\n",
        "    except Exception as e:\n",
        "      print('Failed to delete %s. Reason: %s' % (folder, e))\n",
        "\n",
        "def removeReservedCharsFileName(entityName):\n",
        "  # reservedChars = ['#', '%', '&', '\\{', '\\}', '\\\\', '<', '>', '\\*', '\\?', '/', ' ', '\\$', '!', \"'\", '\"', ':', '@', '\\+', '`', '\\|', '=']\n",
        "  newEntityName = str(entityName)\n",
        "  # for reservedChar in reservedChars:\n",
        "  while re.search(r'[#%&\\{\\}\\\\<>\\*\\?/ \\$!\\'\":@\\+`\\|=]', newEntityName):\n",
        "    newEntityName = re.sub(r'[#%&\\{\\}\\\\<>\\*\\?/ \\$!\\'\":@\\+`\\|=]', \"\", newEntityName)\n",
        "  return(newEntityName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi9-g7wFiLYe"
      },
      "source": [
        "# Run generation pipeline\n",
        "\n",
        "Run all cells to get the concatenated texts, intermediate representations and log files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPOmwVKtnp0"
      },
      "source": [
        "## 1 (Alternative 1) - RDF to PredArg (Using pre-generated WebNLG files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ua2sUZOfshs"
      },
      "outputs": [],
      "source": [
        "# For the moment, you can download the outputs of the conversion and copy the desired inputs of the same split in the str_PredArg_folder (see below).\n",
        "! gdown 1B_8UXCiC71hqqiBhcgTQ1Jg_6Y-Nc5su\n",
        "! unzip /content/00-PredArg-train.zip\n",
        "! rm '/content/00-PredArg-train.zip'\n",
        "\n",
        "! gdown 1YXcDXNS8lnU9EWBbVpYtHIJFQLHqAg0w\n",
        "! unzip /content/00-PredArg-test.zip\n",
        "! rm '/content/00-PredArg-test.zip'\n",
        "\n",
        "! gdown 1L0D3pyUW43qep5C2jvmbdQyzbZesMFOb\n",
        "! unzip /content/00-PredArg-dev.zip\n",
        "! rm '/content/00-PredArg-dev.zip'\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oJ8takwf5RV"
      },
      "outputs": [],
      "source": [
        "# Copy some PredArg structures in the input folder used for generation\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# empty FORGe input folder\n",
        "clear_files(str_PredArg_folder)\n",
        "\n",
        "predArg_conv_folder = '/content/00-PredArg-'+split\n",
        "# predArg_conv_folder = '/content/00-PredArg-dev'\n",
        "# predArg_conv_folder = '/content/00-PredArg-test'\n",
        "# predArg_conv_folder = '/content/00-PredArg-train'\n",
        "list_predArgPaths = glob.glob(os.path.join(predArg_conv_folder, '*.conll'))\n",
        "c = 0\n",
        "for predArgPath in list_predArgPaths:\n",
        "  PAfilename = os.path.split(predArgPath)[-1]\n",
        "  ! cp {predArgPath} '/content/FORGe/structures/00-PredArg/'{PAfilename}\n",
        "  c += 1\n",
        "print('Copied '+str(c)+' files.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lajIWwJHf8Km"
      },
      "outputs": [],
      "source": [
        "# Empty input folder to copy other inputs instead\n",
        "# list_predArgPathsCC = glob.glob(os.path.join('/content/FORGe/structures/00-PredArg/', '*.conll'))\n",
        "# c = 0\n",
        "# for predArgPathCC in list_predArgPathsCC:\n",
        "#   ! rm {predArgPathCC}\n",
        "#   c += 1\n",
        "# print('Removed '+str(c)+' files.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYliSyNnOUCK"
      },
      "source": [
        "## 1 (Alternative 2) - RDF to PredArg (Using XML files in the WebNLG format that you upload in the input_XMLs folder on the left.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjRlUk3JnntV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Pre-processing of input XMls (Upload XML)\n",
        "import codecs\n",
        "import re\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_XML_files = glob.glob(os.path.join(path_input_XML, '*.xml'))\n",
        "\n",
        "months_map = {'January':'01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06', 'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12' }\n",
        "months_list = list(months_map.keys())\n",
        "for XML_file_path in list_XML_files:\n",
        "  print(XML_file_path)\n",
        "  xml_file = codecs.open(XML_file_path, 'r', 'utf-8').readlines()\n",
        "  count_matches = 0\n",
        "  with codecs.open(XML_file_path, 'w', 'utf-8') as fo:\n",
        "    for line in xml_file:\n",
        "      new_line = line\n",
        "      # Reformat dates so they are tagged correctly by XML2PredArg conversion\n",
        "      for month in months_list:\n",
        "        if re.search(month+'_[0-9]_[0-9]{4}', line):\n",
        "          month_num = months_map[month]\n",
        "          new_line = re.subn(month+'_([0-9])_([0-9]{4})', '\\g<2>-'+month_num+'-0\\g<1>', line)[0]\n",
        "          count_matches += 1\n",
        "        elif re.search(month+'_[0-9]{2}_[0-9]{4}', line):\n",
        "          month_num = months_map[month]\n",
        "          new_line = re.subn(month+'_([0-9]{2})_([0-9]{4})', '\\g<2>-'+month_num+'-\\g<1>', line)[0]\n",
        "          count_matches += 1\n",
        "      # Remove the ID prefix to the IDs, breaks the XML2PredArg conversion\n",
        "      if re.search('eid=\"Id', line):\n",
        "        new_line = re.sub('eid=\"Id', 'eid=\"', line)\n",
        "      fo.write(new_line)\n",
        "  print('Replaced '+str(count_matches)+' lines.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3OyRIIjOx1Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Get and write class and gender information\n",
        "import json\n",
        "import glob\n",
        "import xmltodict\n",
        "import shutil\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "def extractTripleElements(dataset, element, existing_list):\n",
        "  \"\"\" Returns a list of subjects, objects or properties extracted from triple sets\"\"\"\n",
        "  n = ''\n",
        "  if element == 'subject':\n",
        "    n = 0\n",
        "  elif element == 'property':\n",
        "    n = 1\n",
        "  elif element == 'object':\n",
        "    n = 2\n",
        "  else:\n",
        "    print('Error, the second argument of extractTripleElements must be \"subject\", \"property\" or \"object\".')\n",
        "  element_list = []\n",
        "  for entry in dataset:\n",
        "    for input_triple in entry:\n",
        "      # print(input_triple)\n",
        "      element_name = input_triple.split(' | ')[n]\n",
        "      # To filter out values that give SPARQL query errors\n",
        "      if not re.search('`', element_name) and not re.search('\"', element_name):\n",
        "        new_element_name = '_'.join(element_name.split(' '))\n",
        "        if new_element_name not in element_list:\n",
        "          if new_element_name not in existing_list:\n",
        "            element_list.append(new_element_name)\n",
        "          # else:\n",
        "          #   print(new_element_name+' was already seen in WebNLG dataset!')\n",
        "  return(element_list)\n",
        "\n",
        "if get_class_gender == 'yes':\n",
        "  # List that contains all unique new triples from the input files\n",
        "  triple_sets_list = []\n",
        "\n",
        "  # Fill triple_sets_list with triples extracted from the input XML\n",
        "  for XML_file_path in list_XML_files:\n",
        "    print('Reading '+XML_file_path+'...')\n",
        "    xml_file = open(XML_file_path, 'r').read()\n",
        "    XML_data = xmltodict.parse(xml_file)\n",
        "\n",
        "    for entry in XML_data['benchmark']['entries']['entry']:\n",
        "      mtriples_list = []\n",
        "      # Get modified triples\n",
        "      if isinstance(entry['modifiedtripleset']['mtriple'], list):\n",
        "        for mtriple in entry['modifiedtripleset']['mtriple']:\n",
        "          mtriples_list.append(mtriple)\n",
        "      else:\n",
        "        mtriples_list.append(entry['modifiedtripleset']['mtriple'])\n",
        "      triple_sets_list.append(mtriples_list)\n",
        "\n",
        "  path_covered_subj = '/content/triples2predArg/classMembership/all_subValues.txt'\n",
        "  path_covered_obj = '/content/triples2predArg/classMembership/all_objValues.txt'\n",
        "\n",
        "  covered_subj_list_raw = codecs.open(path_covered_subj, 'r', 'utf-8').readlines()\n",
        "  covered_obj_list_raw = codecs.open(path_covered_obj, 'r', 'utf-8').readlines()\n",
        "\n",
        "  covered_subj_list = []\n",
        "  covered_obj_list = []\n",
        "  for covered_subj in covered_subj_list_raw:\n",
        "    clean_subj = covered_subj.strip()\n",
        "    covered_subj_list.append(clean_subj)\n",
        "  for covered_obj in covered_obj_list_raw:\n",
        "    clean_obj = covered_obj.strip()\n",
        "    covered_obj_list.append(clean_obj)\n",
        "\n",
        "  # Convert lists of subjets and objects to JSON to pass them as argument\n",
        "  list_subj = sorted(extractTripleElements(triple_sets_list, 'subject', covered_subj_list))\n",
        "  list_obj = sorted(extractTripleElements(triple_sets_list, 'object', covered_obj_list))\n",
        "\n",
        "  json_subj = json.dumps(list_subj)\n",
        "  json_obj = json.dumps(list_obj)\n",
        "  filepath_subj = os.path.join('/content/triples2predArg/classMembership', 'new_subj_values.json')\n",
        "  filepath_obj = os.path.join('/content/triples2predArg/classMembership', 'new_obj_values.json')\n",
        "\n",
        "  with codecs.open(filepath_subj, 'w', 'utf-8') as fo1:\n",
        "    fo1.write(json_subj)\n",
        "  with codecs.open(filepath_obj, 'w', 'utf-8') as fo2:\n",
        "    fo2.write(json_obj)\n",
        "\n",
        "  ! python {path_getClassGenderDBp} {filepath_subj} {filepath_obj}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function for adding grammatical gender/definiteness/number common nouns FR\n",
        "import codecs\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "list_Fem_N = [line.strip() for line in codecs.open('/content/M-FleNS_NLG-Pipeline/resources/FR_feminine_nouns.txt', 'r', 'utf-8').readlines()]\n",
        "list_Fem_N_lower = [fnoun.lower() for fnoun in list_Fem_N]\n",
        "list_Masc_N = [line.strip() for line in codecs.open('/content/M-FleNS_NLG-Pipeline/resources/FR_masculine_nouns.txt', 'r', 'utf-8').readlines()]\n",
        "list_Masc_N_lower = [mnoun.lower() for mnoun in list_Masc_N]\n",
        "\n",
        "# print(list_Fem_N[:20])\n",
        "# print(list_Fem_N_lower[:20])\n",
        "# print(list_Masc_N[:20])\n",
        "# print(list_Masc_N_lower[:20])\n",
        "\n",
        "def get_all_translation(language):\n",
        "  # language not used yet\n",
        "  lines_dbp_subj = codecs.open('/content/triples2predArg/translations/subValues_dbpediaTranslations.txt', 'r', 'utf-8').readlines()\n",
        "  lines_dbp_obj = codecs.open('/content/triples2predArg/translations/objValues_dbpediaTranslations.txt', 'r', 'utf-8').readlines()\n",
        "  lines_gtr_subj = codecs.open('/content/triples2predArg/translations/subValues_googleTranslations.txt', 'r', 'utf-8').readlines()\n",
        "  lines_gtr_obj = codecs.open('/content/triples2predArg/translations/objValues_googleTranslations.txt', 'r', 'utf-8').readlines()\n",
        "\n",
        "  separator_dbp = '*'\n",
        "  separator_gtr = '\\t*\\t'\n",
        "\n",
        "  all_translations = {}\n",
        "  for line_ds in lines_dbp_subj:\n",
        "    if line_ds.split(separator_dbp)[0] not in all_translations.keys():\n",
        "      all_translations[line_ds.split(separator_dbp)[0]] = line_ds.split(separator_dbp)[1].strip()\n",
        "  for line_do in lines_dbp_obj:\n",
        "    if line_do.split(separator_dbp)[0] not in all_translations.keys():\n",
        "      all_translations[line_do.split(separator_dbp)[0]] = line_do.split(separator_dbp)[1].strip()\n",
        "  for line_gs in lines_gtr_subj:\n",
        "    if line_gs.split(separator_gtr)[0] not in all_translations.keys():\n",
        "      all_translations[line_gs.split(separator_gtr)[0]] = line_gs.split(separator_gtr)[1].strip()\n",
        "  for line_go in lines_gtr_obj:\n",
        "    if line_go.split(separator_gtr)[0] not in all_translations.keys():\n",
        "      all_translations[line_go.split(separator_gtr)[0]] = line_go.split(separator_gtr)[1].strip()\n",
        "\n",
        "  return all_translations\n",
        "\n",
        "def is_plural(noun, list_Fem_N, list_Masc_N):\n",
        "  \"\"\"\n",
        "  Determine if a French noun is plural or singular.\n",
        "  Returns True if the noun is plural, False if it is singular.\n",
        "  \"\"\"\n",
        "  noun_to_check = None\n",
        "  # If there is a space in the string, we can't trust that the last word carries the number, so consider it singular\n",
        "  if ' ' in noun:\n",
        "    noun_to_check = noun.split(' ')[0]\n",
        "  elif '_' in noun:\n",
        "    noun_to_check = noun.split('_')[0]\n",
        "  else:\n",
        "    noun_to_check = noun\n",
        "\n",
        "  # Nouns that end in \"s\", \"x\", or \"z\" might be plural but could also be singular, so we check if they are in the fem/masc lists, which only contain singular nouns\n",
        "  if noun_to_check.lower() in list_Fem_N or noun in list_Masc_N:\n",
        "    return False\n",
        "  # If noun starts with an uppercase letter, it's probably a proper noun (although not very true in the WebNLG data...)\n",
        "  elif noun_to_check[0].isupper():\n",
        "    return False\n",
        "  else:\n",
        "    if noun_to_check.endswith(\"s\") or noun_to_check.endswith(\"x\") or noun_to_check.endswith(\"z\"):\n",
        "      # Special endings for French plural nouns\n",
        "      if noun_to_check.endswith((\"eaux\", \"aux\", \"eux\")):\n",
        "        return True  # These are common plural endings\n",
        "      # Nouns ending in just \"s\", \"x\", or \"z\" could be either plural or singular\n",
        "      return True  # Assume it's plural if it ends in \"s\", \"x\", or \"z\"\n",
        "    else:\n",
        "      # If the noun does not end in \"s\", \"x\", or \"z\", it is likely singular\n",
        "      return False\n",
        "\n",
        "def FR_add_gender_definiteness(path_t2p_out_split, path_t2p_out_split_final, list_Fem_N, list_Fem_N_lower, list_Masc_N, list_Masc_N_lower):\n",
        "  dico_months_irish = {\"Eanáir\":\"Janvier\", \"Feabhra\":\"Février\", \"Márta\":\"Mars\", \"Aibreán\":\"Avril\", \"Bealtaine\":\"Mai\", \"Meitheamh\":\"Juin\", \"Iúil\":\"Juillet\", \"Lúnasa\":\"Août\", \"Meán_Fómhair\":\"Septembre\", \"Deireadh_Fómhair\":\"Octobre\", \"Samhain\":\"Novembre\", \"Nollaig\":\"Décembre\"}\n",
        "  dico_months_english = {\"January\":\"Janvier\", \"February\":\"Février\", \"Mars\":\"Mars\", \"April\":\"Avril\", \"May\":\"Mai\", \"June\":\"Juin\", \"July\":\"Juillet\", \"August\":\"Août\", \"September\":\"Septembre\", \"October\":\"Octobre\", \"November\":\"Novembre\", \"December\":\"Décembre\"}\n",
        "  list_conll_files = glob.glob(os.path.join(path_t2p_out_split, '*.conll'))\n",
        "  dico_backup_translations = get_all_translation('FR')\n",
        "  for conll_file in list_conll_files:\n",
        "    # split filename into head and tail\n",
        "    head, tail = os.path.split(conll_file)\n",
        "    # with os.path.join(path_t2p_out_split_final, tail) as fo:\n",
        "    lines_conll = codecs.open(conll_file, 'r', 'utf-8').readlines()\n",
        "    with codecs.open(os.path.join(path_t2p_out_split_final, tail), 'w', 'utf-8') as fo:\n",
        "      for line in lines_conll:\n",
        "        if re.search('(su|o)bj=yes', line):\n",
        "          # Add backup translation if something went wrong during triple2predArg mapping\n",
        "          full_entity = line.split('\\t')[1]\n",
        "          if full_entity in dico_backup_translations.keys():\n",
        "            line = re.sub('^([^\\t]+\\t)[^\\t]+', '\\g<1>'+str(dico_backup_translations.get(full_entity)), line)\n",
        "          # get the word in the second column of the line\n",
        "          word = None\n",
        "          if re.search('_', line.split('\\t')[1]):\n",
        "            word = line.split('\\t')[1].split('_')[0]\n",
        "          elif re.search(' ', line.split('\\t')[1]):\n",
        "            word = line.split('\\t')[1].split(' ')[0]\n",
        "          else:\n",
        "            word = line.split('\\t')[1]\n",
        "          # Get nouns that should be added gender=FEM in the CoNLL\n",
        "          if not re.search('gender=', line):\n",
        "            # list_Fem_N_lower contains all feminine nouns in lower case (same for Masc)\n",
        "            if word.lower() in list_Fem_N_lower and not word.lower() in list_Masc_N_lower:\n",
        "              line = re.sub('^([^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+)', '\\g<1>|gender=FEM', line)\n",
        "          # Add definiteness when needed\n",
        "          if not re.search('definiteness=', line) and not re.search('class=Person', line) and not re.search('class=Band', line) and not re.match(\"(L|l)('|a|e|es)$\", word) and not re.match(\"(U|u)ne*$\", word) and not re.match(\"(D|d)(u|es)$\", word) and not re.match(\"(A|a)$\", word):\n",
        "            # Get nouns that should be added definiteness=DEF in the CoNLL (i.e. if a lowercased entity E is in list_Fem_N, in which supposedly only common nouns are lowercased, then E is likely a common noun, hence likely needs a det)\n",
        "            if word.lower() in list_Fem_N or word.lower() in list_Masc_N:\n",
        "              if re.search('dpos=NP', line):\n",
        "                line = re.sub('dpos=NP', 'definiteness=DEF', line)\n",
        "              else:\n",
        "                line = re.sub('^([^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+)', '\\g<1>|definiteness=DEF', line)\n",
        "          # Add number=PL when needed\n",
        "          if not re.search('number=', line):\n",
        "            if is_plural(word, list_Fem_N, list_Masc_N) == True:\n",
        "              line = re.sub('^([^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+)', '\\g<1>|number=PL', line)\n",
        "          # Replace Irish month names by french names\n",
        "          # print('Processing line '+str(line))\n",
        "          for month_i in dico_months_irish.keys():\n",
        "            if month_i in line.split('\\t')[1]:\n",
        "              # for Irish, let's just replace any month label by the french one (note there are months with an underscore in the middle).\n",
        "              line = re.sub(month_i, dico_months_irish.get(month_i), line)\n",
        "          for month_e in dico_months_english.keys():\n",
        "            if month_e in line.split('\\t')[1]:\n",
        "              # for English, get rid of commas if any; also check is we actually have a date to avoid possible errors (e.g. a month in a book name)\n",
        "              line = re.sub(month_e+',*(_[0-9]{4})', dico_months_english.get(month_e)+'\\g<1>', line)\n",
        "        fo.write(line)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Cofrbw2U5dlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lVnZKzuPFhx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create FORGe input file in conll format (TEMPORARY FOR FR: upload manually '241008_WebNLG23_FR.conll' and the translation files, and use 'GA' block in this cell)\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# empty FORGe input folder\n",
        "clear_files(str_PredArg_folder)\n",
        "\n",
        "language_t2p = language.lower()\n",
        "path_t2p_out = os.path.join(triple2predArg, 'out/')\n",
        "clear_files(path_t2p_out)\n",
        "\n",
        "path_t2p_out_split = os.path.join(triple2predArg, 'out_split/')\n",
        "if not os.path.exists(path_t2p_out_split):\n",
        "  os.makedirs(path_t2p_out_split)\n",
        "else:\n",
        "  clear_files(path_t2p_out_split)\n",
        "\n",
        "path_t2p_out_split_final = os.path.join(triple2predArg, 'out_split_final/')\n",
        "if not os.path.exists(path_t2p_out_split_final):\n",
        "  os.makedirs(path_t2p_out_split_final)\n",
        "else:\n",
        "  clear_files(path_t2p_out_split_final)\n",
        "\n",
        "name_conll_templates = ''\n",
        "name_properties_list = ''\n",
        "\n",
        "if language == 'GA':\n",
        "  # name_conll_templates_outdated = '221130_WebNLG23_GA.conll'\n",
        "  # name_properties_list_outdated = '231027-WebNLG23_EN-GA_properties.txt'\n",
        "  # name_conll_templates = '241008_WebNLG23_FR.conll'\n",
        "  name_conll_templates = '240202_WebNLG23_GA.conll'\n",
        "  name_properties_list = '240202_WebNLG23_EN-GA_properties.txt'\n",
        "# elif language == 'FR':\n",
        "  # For FR for now: upload manually '241008_WebNLG23_FR.conll' and the translation files with the same name as the GA files (FR is not yet accepted by triples2predarg)\n",
        "  # IF UPDATED: ALSO UPDATE LANGUAGE FOR LANGUAGE-SPECIFIC PROCESSING JUST AFTER THE CONVERSION\n",
        "  # name_conll_templates = '241008_WebNLG23_FR.conll'\n",
        "  # name_properties_list = '240202_WebNLG23_EN-GA_properties.txt'\n",
        "else:\n",
        "  # name_conll_templates = '230528-WebNLG23_EN.conll'\n",
        "  name_conll_templates = '240202_WebNLG23_EN.conll'\n",
        "  name_properties_list = '240202_WebNLG23_EN-GA_properties.txt'\n",
        "\n",
        "# newEntityName = removeReservedCharsFileName(entity_name)\n",
        "\n",
        "for XML_file_path in list_XML_files:\n",
        "  inputFilename = XML_file_path.rsplit('/', 1)[1].rsplit('.', 1)[0]\n",
        "  # Copy input files to triples2predArg repo\n",
        "  shutil.copy(XML_file_path, os.path.join(triple2predArg, inputFilename)+'.xml')\n",
        "\n",
        "  # Convert xml into predArg\n",
        "  print('Converting '+inputFilename+' to PredArg...')\n",
        "  !java -jar '/content/triples2predArg/webNLG_triples2conll.jar' '/content/triples2predArg/' {name_conll_templates} {name_properties_list} {path_t2p_out} {language_t2p} {inputFilename}  # -> \"log.txt\"\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  # File splitting: the generator cannot process files that are too big, so they need to be split. For regular-sized inputs, 450 inputs per file should do.\n",
        "  # Whether split or not, the output files will be copied in the folder in [5]\n",
        "  # Parameters:\n",
        "  # [1] path to input folder\n",
        "  # [2] encoding of input files\n",
        "  # [3] number of structures per file\n",
        "  # [4] split once ('first'), or every time the threshold in [3] is reached ('all')\n",
        "  # [5] path to temp folder used to store split files\n",
        "  ! python {path_splitFiles} {path_t2p_out} 'utf-8' 300 'all' {path_t2p_out_split}\n",
        "\n",
        "  # Add language-specific info, such as gender, definiteness info; creates new path '/content/triples2predArg/out_split_final'\n",
        "  # Next line should be FR, cheating for now\n",
        "  if language == 'GA':\n",
        "    FR_add_gender_definiteness(path_t2p_out_split, path_t2p_out_split_final, list_Fem_N, list_Fem_N_lower, list_Masc_N, list_Masc_N_lower)\n",
        "  else:\n",
        "    if os.path.exists(path_t2p_out_split_final):\n",
        "      shutil.rmtree(path_t2p_out_split_final)\n",
        "    shutil.copytree(path_t2p_out_split, path_t2p_out_split_final)\n",
        "\n",
        "  # Copy conll file from final conversion folder to FORGe input folder\n",
        "  # The following \"if\" is not needed but I keep it in case we don't use splitFiles in the future.\n",
        "  if len(os.listdir(path_t2p_out_split_final)) == 0:\n",
        "    shutil.copy(os.path.join(path_t2p_out, inputFilename+'_'+language_t2p+'.conll'), str_PredArg_folder)\n",
        "    print('Copied files from '+str(path_t2p_out))\n",
        "  else:\n",
        "    list_conll_files = glob.glob(os.path.join(path_t2p_out_split_final, '*.conll'))\n",
        "    for conll_file in list_conll_files:\n",
        "      # conllFilename = conll_file.rsplit('/', 1)[1].rsplit('.', 1)[0]\n",
        "      shutil.copy(conll_file, str_PredArg_folder)\n",
        "    print('Copied files from '+str(path_t2p_out_split_final))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check for missing mappings\n",
        "path_debug_triples2predArg = '/content/triples2predArg/out/test_missingProperties.txt'\n",
        "\n",
        "pdt_lines = codecs.open(path_debug_triples2predArg, 'r', 'utf-8').readlines()\n",
        "\n",
        "if len(pdt_lines) == 0:\n",
        "  print(f'All input properties were mapped successfully!')\n",
        "else:\n",
        "  print(f'CRITICAL WARNING: the mapping of some input properties was not found!')\n",
        "  for pdt_line in pdt_lines:\n",
        "    print(f'  {pdt_line.strip()}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XoJtNpRW2GDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download conll inputs\n",
        "download_inputs = 'yes'#@param['yes', 'no']\n",
        "split = 'dev' #@param ['train', 'dev', 'test']\n",
        "\n",
        "# language = 'FR'\n",
        "if download_inputs == 'yes':\n",
        "  from google.colab import files\n",
        "  zip_name_log = '/content/00-PredArg-'+split+'.zip'\n",
        "  !zip -r {zip_name_log} /content/triples2predArg/out_split_final\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  files.download(zip_name_log)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UYKG-fTxdjQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0l_N2UGtrNx"
      },
      "source": [
        "## 2 - PredArg to uninflected text (FORGe via M-FleNS pipeline code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_DsIRYArBq8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Launch generation process\n",
        "# ! python '/content/M-FleNS_NLG-Pipeline/code/M-FleNS.py' {language} {split} {group_modules_prm} {PredArg_Normalisation} {PredArg_AggregationMark} {PredArg_Aggregation} {PredArg_PoSTagging} {PredArg_CommStructuring} {DSynt_Structuring} {SSynt_Structuring} {SSynt_Aggregation} {RE_Generation} {DMorph_AgreementsLinearisation} {SMorph_Processing} {FORGe_input_folder} {path_MATE} {path_props_resources_template} {path_props_levels} {path_props} {str_PredArg_folder} {str_PredArgNorm_folder} {str_PredArgAggMark_folder} {str_PredArgAgg_folder} {str_PredArgPoS_folder} {str_PredArgComm_folder} {str_DSynt_folder} {str_SSynt_folder} {str_SSyntAgg_folder} {str_REG_folder} {str_DMorphLin_folder} {str_SMorphText_folder} {log_folder}\n",
        "! python {path_MFleNS} {language} {split} {group_modules_prm} {PredArg_Normalisation} {PredArg_AggregationMark} {PredArg_Aggregation} {PredArg_PoSTagging} {PredArg_CommStructuring} {DSynt_Structuring} {SSynt_Structuring} {SSynt_Aggregation} {RE_Generation} {DMorph_AgreementsLinearisation} {SMorph_Processing} {FORGe_input_folder} {path_MATE} {path_props_resources_template} {path_props_levels} {path_props} {str_PredArg_folder} {str_PredArgNorm_folder} {str_PredArgAggMark_folder} {str_PredArgAgg_folder} {str_PredArgPoS_folder} {str_PredArgComm_folder} {str_DSynt_folder} {str_SSynt_folder} {str_SSyntAgg_folder} {str_REG_folder} {str_DMorphLin_folder} {str_SMorphText_folder} {log_folder}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzXG50NysYe8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Check outputs and copy files to morph folder if GA\n",
        "import codecs\n",
        "\n",
        "# # Read original check script\n",
        "# check_script = open(path_checkOutputs, 'r')\n",
        "# lines_check_script = check_script.readlines()\n",
        "# # Update check script\n",
        "# with codecs.open(path_checkOutputs, 'w', 'utf-8') as f:\n",
        "#   for line in lines_check_script:\n",
        "#     if line.startswith('log_folder = sys.argv[3]'):\n",
        "#       f.write('log_folder = sys.argv[3]\\ntemp_input_folder_morph = sys.argv[4]\\nlanguage = sys.argv[5]\\n')\n",
        "#     elif line.startswith('            count_perLevel.append(count)\\n'):\n",
        "#       f.write('            count_perLevel.append(count)\\n            if language == \"GA\":\\n              shutil.copy(new_file_path, temp_input_folder_morph)\\n')\n",
        "#     else:\n",
        "#       f.write(line)\n",
        "\n",
        "! python {path_checkOutputs} {str_PredArg_folder} {str_SMorphText_folder} {log_folder} {temp_input_folder_morph} {language}\n",
        "\n",
        "if not language == 'GA':\n",
        "  clear_folder(os.path.join(temp_input_folder_morph, split))\n",
        "  # For GA, files are copied from the python code called above\n",
        "  if concatenate_output_files == 'yes':\n",
        "    ! python {path_concatenate} {str_SMorphText_folder} {temp_input_folder_morph} {split}\n",
        "  else:\n",
        "    ! python {path_concatenate} {str_SMorphText_folder} {temp_input_folder_morph} {split}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iPz_wQhte3q"
      },
      "source": [
        "## 3 - Morphology processing (Irish NLP Tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqqbgzs3zqqj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Use if you upload structures generated from another pipeline instead of using the previous cells\n",
        "# import shutil\n",
        "# clear_folder('/content/FORGe/structures')\n",
        "# ! unzip /content/FORGe-train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TFQOgvG0laB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Process raw FORGe output and format it for Morphology\n",
        "\n",
        "# Read original check script\n",
        "if os.path.isfile('/content/FORGe/log/summary.txt'):\n",
        "  FORGe_log = open('/content/FORGe/log/summary.txt', 'r')\n",
        "  lines_log = FORGe_log.readlines()\n",
        "  # Get number of expected texts\n",
        "  count_strs_all_FORGe = 0\n",
        "  for line in lines_log:\n",
        "    if line.startswith('Outputs: '):\n",
        "      count_strs_all_FORGe = int(line.strip().split('Outputs: ')[-1])\n",
        "\n",
        "  print('Expected texts: '+str(count_strs_all_FORGe)+'.\\n')\n",
        "\n",
        "if language == 'GA':\n",
        "  ! python {path_FORGe2Morph} {language} {temp_input_folder_morph} {morph_input_folder}\n",
        "  clear_files(temp_input_folder_morph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uPBqNqjkueAk"
      },
      "outputs": [],
      "source": [
        "#@title Call morph generator\n",
        "# v3 (fast, ~2sec/450 texts)\n",
        "\n",
        "# Run the morphology generation\n",
        "from IPython.display import HTML, display\n",
        "import progressbar\n",
        "import glob\n",
        "import codecs\n",
        "from termcolor import colored\n",
        "import re\n",
        "\n",
        "show_input = False #@param {type:\"boolean\"}\n",
        "\n",
        "if language == 'GA':\n",
        "  clear_files(morph_output_folder)\n",
        "  # To store how many texts we have in each file (used to )\n",
        "  count_strs_all_Morph = []\n",
        "  for filepath in sorted(glob.glob(os.path.join(morph_input_folder, '*.*'))):\n",
        "    count_strs_all = 0\n",
        "    head, tail = os.path.split(filepath)\n",
        "    filename = tail.rsplit('.')[0]\n",
        "    print('Processing '+filename)\n",
        "    fo = codecs.open(morph_output_folder+'/'+filename+'_out.txt', 'w', 'utf-8')\n",
        "    list_inflected_words = ! cat {filepath} | {morph_folder_name}'/flookup' -a {morph_folder_name}'/allgen.fst'\n",
        "    # print(list_inflected_words)\n",
        "\n",
        "    # Create a variable to store the outputs\n",
        "    text = ''\n",
        "    # morph returns this as list_inflected_words: ['imir+Verb+Vow+PresInd\\timríonn', '', 'Agremiação_Sportiva_Arapiraquense+Noun+Masc+Com+Pl\\t+?', '', ',\\t+?',...]\n",
        "    for word in list_inflected_words:\n",
        "      empty = 'yes'\n",
        "      input_string = ''\n",
        "      morph_returned = ''\n",
        "      morph_backup = ''\n",
        "      if re.search('\\t', word):\n",
        "        # for every space an empty string is returned; we'll ignore them later. Between two consecutive texts there is a simple \"\\t\" with nothing around. I use this to introduce linebreaks later.\n",
        "        empty = 'no'\n",
        "        input_string = word.split('\\t')[0]\n",
        "        morph_returned = word.split('\\t')[1]\n",
        "        if re.search('\\+', word):\n",
        "          morph_backup = input_string.split('+', 1)[0]\n",
        "        else:\n",
        "          morph_backup = input_string\n",
        "      out_line = ''\n",
        "      # Create each output line with the required contents\n",
        "      if show_input == True:\n",
        "        if empty == 'no':\n",
        "          if morph_returned == '':\n",
        "            if input_string == '':\n",
        "              out_line = out_line + '\\n'\n",
        "              count_strs_all += 1\n",
        "          else:\n",
        "            out_line = out_line + input_string + ': ' +'\\x1b[5;30;47m'+morph_returned+'\\x1b[0m'+'\\n'\n",
        "      else:\n",
        "        if empty == 'no':\n",
        "          if morph_returned == '+?':\n",
        "            out_line = out_line + morph_backup + ' '\n",
        "          # If the line is empty, add a line break (empty lines separate different texts in the input)\n",
        "          elif morph_returned == '':\n",
        "            if input_string == '':\n",
        "              out_line = out_line + '\\n'\n",
        "              count_strs_all += 1\n",
        "          else:\n",
        "            out_line = out_line + morph_returned + ' '\n",
        "      # add line to the other lines of the same file\n",
        "      text = text + out_line\n",
        "\n",
        "    # print('\\n----------------------\\n'+text+'\\n')\n",
        "    count_strs_all_Morph.append(count_strs_all)\n",
        "    fo.write(text+'\\n')\n",
        "    fo.close()\n",
        "\n",
        "  # Check\n",
        "  if os.path.isfile('/content/FORGe/log/summary.txt'):\n",
        "    with codecs.open('/content/FORGe/log/summary.txt', 'a', 'utf-8') as fo:\n",
        "      fo.write('\\nMorphology debug\\n==================\\n\\n')\n",
        "      if not sum(count_strs_all_Morph) == count_strs_all_FORGe:\n",
        "        print('\\nERROR! Mismatch with FORGe outputs!')\n",
        "        fo.write('ERROR! Mismatch with FORGe outputs!\\n')\n",
        "      print('\\nThere are '+str(sum(count_strs_all_Morph))+' texts.')\n",
        "      fo.write('There are '+str(sum(count_strs_all_Morph))+' texts.\\n')\n",
        "      print('Texts per file: '+str(count_strs_all_Morph))\n",
        "      fo.write('Texts per file: '+str(count_strs_all_Morph)+'\\n')\n",
        "      fo.write('---------------------------------\\n')\n",
        "  clear_files(morph_input_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSsBqWCAuxyE"
      },
      "source": [
        "## 4 - Output post-processing and packaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODAI93StJjVd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Process texts\n",
        "prefinal_output_folder = ''\n",
        "\n",
        "if language == 'GA':\n",
        "  prefinal_output_folder = morph_output_folder\n",
        "else:\n",
        "  prefinal_output_folder = os.path.join(temp_input_folder_morph, split)\n",
        "\n",
        "! python {path_postProc} {language} {prefinal_output_folder}\n",
        "\n",
        "# Check\n",
        "list_filepaths = glob.glob(os.path.join(prefinal_output_folder, '*_postproc.txt'))\n",
        "count_strs_all_postproc = []\n",
        "for filepath in sorted(list_filepaths):\n",
        "  count_strs_all = 0\n",
        "  head, tail = os.path.split(filepath)\n",
        "  fd = codecs.open(filepath, 'r', 'utf-8')\n",
        "  lines = fd.readlines()\n",
        "  x = 0\n",
        "  for line in lines:\n",
        "    if not line == '\\n':\n",
        "      count_strs_all += 1\n",
        "    x += 1\n",
        "  count_strs_all_postproc.append(count_strs_all)\n",
        "\n",
        "if os.path.isfile('/content/FORGe/log/summary.txt'):\n",
        "  with codecs.open('/content/FORGe/log/summary.txt', 'a', 'utf-8') as fo:\n",
        "    fo.write('\\nPost-processing debug\\n==================\\n\\n')\n",
        "    if not sum(count_strs_all_postproc) == count_strs_all_FORGe:\n",
        "      print('\\nERROR! Mismatch with FORGe outputs!')\n",
        "      fo.write('ERROR! Mismatch with FORGe outputs!\\n')\n",
        "    print('\\nThere are '+str(sum(count_strs_all_postproc))+' texts.')\n",
        "    fo.write('There are '+str(sum(count_strs_all_postproc))+' texts.\\n')\n",
        "    print('Texts per file: '+str(count_strs_all_postproc))\n",
        "    fo.write('Texts per file: '+str(count_strs_all_postproc)+'\\n')\n",
        "    fo.write('---------------------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfQ08wHOu8eZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Concatenate files\n",
        "\n",
        "# list_clean_outputs = glob.glob(os.path.join(morph_output_folder, '*_out_postproc.txt'))\n",
        "list_clean_outputs = ''\n",
        "if language == 'GA':\n",
        "  list_clean_outputs = glob.glob(os.path.join(morph_output_folder, '*_out_postproc.txt'))\n",
        "else:\n",
        "  list_clean_outputs = glob.glob(os.path.join(temp_input_folder_morph, split, '*_postproc.txt'))\n",
        "print(list_clean_outputs)\n",
        "\n",
        "filename = 'all_'+language+'_'+split+'_out.txt'\n",
        "\n",
        "with codecs.open(filename, 'w', 'utf-8') as outfile:\n",
        "  # Files need to be sorted to be concatenated in the right order\n",
        "  for fname in sorted(list_clean_outputs):\n",
        "    print('Processing '+fname)\n",
        "    with open(fname) as infile:\n",
        "      outfile.write(infile.read())\n",
        "\n",
        "# Check\n",
        "if os.path.isfile('/content/FORGe/log/summary.txt'):\n",
        "  with codecs.open('/content/FORGe/log/summary.txt', 'a', 'utf-8') as fo:\n",
        "    fo.write('\\nConcatenate debug\\n==================\\n\\n')\n",
        "    count_texts_all = len(codecs.open(filename).readlines())\n",
        "    if not count_texts_all == count_strs_all_FORGe:\n",
        "      print('\\nERROR! Mismatch with FORGe outputs!')\n",
        "      fo.write(('ERROR! Mismatch with FORGe outputs!\\n'))\n",
        "    print('\\nThere are '+str(count_texts_all)+' texts.')\n",
        "    fo.write('There are '+str(count_texts_all)+' texts.\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRClGzfcJ5Nv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Zip and download FORGe output folder with intermediate representations\n",
        "download_outputs = 'yes'#@param['yes', 'no']\n",
        "\n",
        "if download_outputs == 'yes':\n",
        "  from google.colab import files\n",
        "  zip_name_inter = '/content/WebNLG_['+language+']_['+split+']_allLevels.zip'\n",
        "  !zip -r {zip_name_inter} /content/FORGe/structures\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  files.download(zip_name_inter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_yzCQGBshN1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Zip and download FORGe log files folder\n",
        "\n",
        "download_logfiles = 'no'#@param['yes', 'no']\n",
        "\n",
        "if download_logfiles == 'yes':\n",
        "  from google.colab import files\n",
        "  zip_name_log = '/content/WebNLG_['+language+']_['+split+']_logs.zip'\n",
        "  !zip -r {zip_name_log} /content/FORGe/log\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  files.download(zip_name_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQbOtHkz74xD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Zip and download conll inputs\n",
        "download_inputs = 'no'#@param['yes', 'no']\n",
        "\n",
        "if download_inputs == 'yes':\n",
        "  from google.colab import files\n",
        "  zip_name_log = '/content/WebNLG_['+language+']_inputs.zip'\n",
        "  !zip -r {zip_name_log} /content/FORGe/structures/00-PredArg\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  files.download(zip_name_log)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! unzip /content/WebNLG_[EN]_[test]_allLevels.zip"
      ],
      "metadata": {
        "id": "vYZKL3zmEjCt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DhPOmwVKtnp0",
        "t0l_N2UGtrNx",
        "0iPz_wQhte3q"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNdpVlNiA9+igu43o17MolX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}